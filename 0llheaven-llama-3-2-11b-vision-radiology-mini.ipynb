{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12496500,"sourceType":"datasetVersion","datasetId":7886436}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 0llheaven/Llama-3.2-11B-Vision-Radiology-mini\nhttps://huggingface.co/0llheaven/Llama-3.2-11B-Vision-Radiology-mini","metadata":{}},{"cell_type":"code","source":"!pip install unsloth\n!pip install torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:16:38.676778Z","iopub.execute_input":"2025-07-17T10:16:38.677611Z","iopub.status.idle":"2025-07-17T10:16:45.455133Z","shell.execute_reply.started":"2025-07-17T10:16:38.677565Z","shell.execute_reply":"2025-07-17T10:16:45.454415Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.7.3)\nRequirement already satisfied: unsloth_zoo>=2025.7.4 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2025.7.4)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.7.1)\nRequirement already satisfied: xformers>=0.0.27.post2 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.0.31.post1)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.46.1)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.3.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (25.0)\nRequirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.9.26)\nRequirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.52.4)\nRequirement already satisfied: datasets<4.0.0,>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.6.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (7.0.0)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.26.4)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.8.1)\nRequirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.19.1)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.20.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.33.1)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.34.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.22.1)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (2.32.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<4.0.0,>=3.4.1->unsloth) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->unsloth) (2.4.1)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth) (0.21.2)\nRequirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.7.4->unsloth) (25.1.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.7.4->unsloth) (11.2.1)\nRequirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.7.4->unsloth) (0.19.0)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (14.0.0)\nRequirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (1.7.2)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (3.12.13)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth) (2025.6.15)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->unsloth) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->unsloth) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets<4.0.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth) (1.20.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->unsloth) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth) (1.17.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch) (9.5.1.17)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\nRequirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.1)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from unsloth import FastVisionModel\nfrom PIL import Image\nimport numpy as np\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:17:30.773231Z","iopub.execute_input":"2025-07-17T10:17:30.773584Z","iopub.status.idle":"2025-07-17T10:17:58.963342Z","shell.execute_reply.started":"2025-07-17T10:17:30.773552Z","shell.execute_reply":"2025-07-17T10:17:58.962560Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-07-17 10:17:38.196949: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752747458.225167     167 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752747458.233709     167 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:19:06.537399Z","iopub.execute_input":"2025-07-17T10:19:06.537771Z","iopub.status.idle":"2025-07-17T10:19:06.542080Z","shell.execute_reply.started":"2025-07-17T10:19:06.537748Z","shell.execute_reply":"2025-07-17T10:19:06.541499Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:19:08.403336Z","iopub.execute_input":"2025-07-17T10:19:08.403973Z","iopub.status.idle":"2025-07-17T10:19:08.409328Z","shell.execute_reply.started":"2025-07-17T10:19:08.403947Z","shell.execute_reply":"2025-07-17T10:19:08.408569Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"model, tokenizer = FastVisionModel.from_pretrained(\n    \"0llheaven/Llama-3.2-11B-Vision-Radiology-mini\",\n    load_in_4bit=True,\n    use_gradient_checkpointing=\"unsloth\",\n)\n\nFastVisionModel.for_inference(model)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:19:09.695237Z","iopub.execute_input":"2025-07-17T10:19:09.695548Z","iopub.status.idle":"2025-07-17T10:22:31.512324Z","shell.execute_reply.started":"2025-07-17T10:19:09.695524Z","shell.execute_reply":"2025-07-17T10:22:31.511495Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.7.3: Fast Mllama patching. Transformers: 4.52.4.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7267c48ee0a74b0aaf004424f11b2764"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f69222cee01f41b9b48b79e7222e997f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49afdd7af905473792156dc66704e571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e154840d104fffb5900e045b8f47aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f06723254a3b4a7797e7736eb2dc8e4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4536ce1d6c43208e9125c775e517d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6dee1830a18435381c4a5cc477d0f3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09d3162e060e410ba81d2454b11bf6ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf4e3a53a894a309964fbabba0afa75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ec914fafc8418b9a7e23c2ac8b4a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ff8dee2ec4b4a91b219e10331f88c8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1e8185914dd4623a9d509c55766fe75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52504eaad1e04cf2aef72400f34107e5"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"MllamaForConditionalGeneration(\n  (model): MllamaModel(\n    (vision_model): MllamaVisionModel(\n      (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), padding=valid, bias=False)\n      (gated_positional_embedding): MllamaPrecomputedPositionEmbedding(\n        (tile_embedding): Embedding(9, 8197120)\n      )\n      (pre_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n        (embedding): Embedding(9, 5120)\n      )\n      (post_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n        (embedding): Embedding(9, 5120)\n      )\n      (layernorm_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (layernorm_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n      (transformer): MllamaVisionEncoder(\n        (layers): ModuleList(\n          (0-31): 32 x MllamaVisionEncoderLayer(\n            (self_attn): MllamaVisionAttention(\n              (q_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n              (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n              (v_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n              (o_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n            )\n            (mlp): MllamaVisionMLP(\n              (activation_fn): GELUActivation()\n              (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n              (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n            )\n            (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (global_transformer): MllamaVisionEncoder(\n        (layers): ModuleList(\n          (0-7): 8 x MllamaVisionEncoderLayer(\n            (self_attn): MllamaVisionAttention(\n              (q_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n              (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n              (v_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n              (o_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n            )\n            (mlp): MllamaVisionMLP(\n              (activation_fn): GELUActivation()\n              (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n              (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n            )\n            (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n            (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n    )\n    (language_model): MllamaTextModel(\n      (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n      (layers): ModuleList(\n        (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n          (self_attn): MllamaTextSelfAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (3): MllamaCrossAttentionDecoderLayer(\n          (cross_attn): MllamaTextCrossAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n          (self_attn): MllamaTextSelfAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (8): MllamaCrossAttentionDecoderLayer(\n          (cross_attn): MllamaTextCrossAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n          (self_attn): MllamaTextSelfAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (13): MllamaCrossAttentionDecoderLayer(\n          (cross_attn): MllamaTextCrossAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n          (self_attn): MllamaTextSelfAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (18): MllamaCrossAttentionDecoderLayer(\n          (cross_attn): MllamaTextCrossAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n          (self_attn): MllamaTextSelfAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (23): MllamaCrossAttentionDecoderLayer(\n          (cross_attn): MllamaTextCrossAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n          (self_attn): MllamaTextSelfAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (28): MllamaCrossAttentionDecoderLayer(\n          (cross_attn): MllamaTextCrossAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n          (self_attn): MllamaTextSelfAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (33): MllamaCrossAttentionDecoderLayer(\n          (cross_attn): MllamaTextCrossAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n          (self_attn): MllamaTextSelfAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (38): MllamaCrossAttentionDecoderLayer(\n          (cross_attn): MllamaTextCrossAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n        (39): MllamaSelfAttentionDecoderLayer(\n          (self_attn): MllamaTextSelfAttention(\n            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          )\n          (mlp): MllamaTextMLP(\n            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (act_fn): SiLU()\n          )\n          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n        )\n      )\n      (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n      (rotary_emb): MllamaRotaryEmbedding()\n    )\n    (multi_modal_projector): Linear(in_features=7680, out_features=4096, bias=True)\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def predict_radiology_description(image, instruction):\n    try:\n        messages = [{\"role\": \"user\", \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction}\n        ]}]\n        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n\n        inputs = tokenizer(\n            image,\n            input_text,\n            add_special_tokens=False,\n            return_tensors=\"pt\",\n        ).to(device)\n\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=1.5,\n            min_p=0.1\n        )\n\n        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        return generated_text.replace(\"assistant\", \"\\n\\nassistant\").strip()\n    except Exception as e:\n        return f\"Error: {str(e)}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:26:12.092915Z","iopub.execute_input":"2025-07-17T10:26:12.093223Z","iopub.status.idle":"2025-07-17T10:26:12.099524Z","shell.execute_reply.started":"2025-07-17T10:26:12.093191Z","shell.execute_reply":"2025-07-17T10:26:12.098702Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"image_path = '/kaggle/input/xray-image/broken-hand.jpg'\ninstruction = 'are there fractures present?'\n\nimage = Image.open(image_path).convert(\"RGB\")\noutput = predict_radiology_description(image, instruction)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:23:07.339072Z","iopub.execute_input":"2025-07-17T10:23:07.339367Z","iopub.status.idle":"2025-07-17T10:23:40.294242Z","shell.execute_reply.started":"2025-07-17T10:23:07.339342Z","shell.execute_reply":"2025-07-17T10:23:40.293472Z"}},"outputs":[{"name":"stdout","text":"user\n\nare there fractures present?\n\nassistant\n\nYes, there are fractures present. The image shows a broken arm with two visible fractures, one in the radius bone (the long bone in the forearm) and one in the ulna bone (the shorter bone in the forearm).\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"image_path = '/kaggle/input/xray-image/cancer-xray.jpg'\ninstruction = 'is there any presence of cancer or tumors? what is the diagnosis?'\n\nimage = Image.open(image_path).convert(\"RGB\")\noutput = predict_radiology_description(image, instruction)\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:23:57.305643Z","iopub.execute_input":"2025-07-17T10:23:57.305945Z","iopub.status.idle":"2025-07-17T10:24:09.760314Z","shell.execute_reply.started":"2025-07-17T10:23:57.305921Z","shell.execute_reply":"2025-07-17T10:24:09.759617Z"}},"outputs":[{"name":"stdout","text":"user\n\nis there any presence of cancer or tumors? what is the diagnosis?\n\nassistant\n\nThere is a visible mass in the right upper lobe of the lung on this X-ray. The presence of a mass in the lung, as shown in this image, may indicate lung cancer or other serious health complications, including infection and other pulmonary conditions.\n\nAs lung cancer and tumors can be life threatening, it is crucial to follow up with a healthcare provider to confirm the diagnosis and obtain further imaging and medical care, including treatment.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"image_path = '/kaggle/input/xray-image/broken-leg.jpg'\ninstruction = 'what is the diagnosis?'\n\nimage = Image.open(image_path).convert(\"RGB\")\noutput = predict_radiology_description(image, instruction)\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T10:26:15.836667Z","iopub.execute_input":"2025-07-17T10:26:15.837493Z","iopub.status.idle":"2025-07-17T10:26:36.093134Z","shell.execute_reply.started":"2025-07-17T10:26:15.837458Z","shell.execute_reply":"2025-07-17T10:26:36.092259Z"}},"outputs":[{"name":"stdout","text":"user\n\nwhat is the diagnosis?\n\nassistant\n\nThis X-ray image shows two broken legs, both of which are compound fractures, one of the right femur and one of the right tibia. The fracture of the right femur appears to have some degree of movement between the pieces, indicating potential dislocation and soft tissue damage. Additionally, a possible bone spur or old fracture line can be observed in the lower part of the femur, suggesting a possible history of trauma or a previous fracture. The tibia appears to have more movement between the bone fragments and shows signs of possible soft tissue injury around the right foot as well. These observations imply that this person would likely be hospitalized with possible surgery, an orthopedic specialist would need to further assess these fractures before forming a definitive diagnosis.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}